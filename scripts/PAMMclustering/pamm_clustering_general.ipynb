{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import datasets, cluster, mixture\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from gmplabtools.pamm.lib.dimensionality import DataSampler\n",
    "from gmplabtools.pamm.lib.tools import GMMPredict\n",
    "from gmplabtools.pamm.pamm_commander import PammCommander\n",
    "from gmplabtools.pamm.lib.clustering_tools import calculate_adjacency, adjancency_dendrogram\n",
    "from gmplabtools.pamm.lib.transition_rates import ClusterRates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory of stored pca files\n",
    "dir_ = '/mydir'\n",
    "\n",
    "# individuals pca sets trained from the whole set\n",
    "SYS1pca = np.loadtxt(dir_+'/bylayer273K.pca')\n",
    "SYS2pca = np.loadtxt(dir_+'/bylayer293K.pca')\n",
    "SYS3pca = np.loadtxt(dir_+'/bylayer323K.pca')\n",
    "SYS4pca = np.loadtxt(dir_+'/bylayer333K.pca')\n",
    "SYS5pca = np.loadtxt(dir_+'/bylayer353K.pca')\n",
    "\n",
    "# whole (merged) set of bylayers\n",
    "ALLpca = np.loadtxt(dir_+'/all_bylayers.pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pamm input\n",
    "default_inputs = dict(\n",
    "    # cluster\n",
    "    distance = \"minkowski\",\n",
    "    size = 2000,\n",
    "    p = 2,\n",
    "    generate_grid = True,\n",
    "    savegrid = \"grid_data\",\n",
    "    # cluster inputs\n",
    "    d = 5,\n",
    "    fspread = 0.3,\n",
    "    ngrid = 2000,\n",
    "    qs = 1,\n",
    "    o = \"pamm\",\n",
    "    trajectory = dir_+'/all_bylayers.pca',\n",
    "#     readgrid = \"grid_data\",\n",
    "    merger = 0.001,\n",
    "    bootstrap = 73\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_cluster = [\n",
    "    (ALLpca, {}),\n",
    "]\n",
    "\n",
    "datasets_predict = [\n",
    "    (SYS1pca, {'sys' : '273K', 'nm_frame' : 1152}),\n",
    "    (SYS2pca, {'sys' : '293K', 'nm_frame' : 1152}),\n",
    "    (SYS3pca, {'sys' : '323K', 'nm_frame' : 1152}),\n",
    "    (SYS4pca, {'sys' : '333K', 'nm_frame' : 1152}),\n",
    "    (SYS5pca, {'sys' : '353K', 'nm_frame' : 1152})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PAMM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on the selected grid of the whole dataset\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets_cluster):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_inputs.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    # Clustering\n",
    "    p = PammCommander(params)\n",
    "    print('\\n#-----------------------------------------------')\n",
    "    print(p.command_parser)\n",
    "    \n",
    "    print('\\nRUNNING Clustering')\n",
    "    t0 = time.time()\n",
    "    p.run()\n",
    "    t1 = time.time()\n",
    "    print('TIME= '+str(np.round(t1-t0, 2))+' s \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the individuals set and outputs\n",
    "\n",
    "cluster_output = {}\n",
    "grid_cluster = {}\n",
    "prob_output = {}\n",
    "bootstr_output = {}\n",
    "systnames = []\n",
    "for i_dataset,dataset in enumerate(datasets_predict):\n",
    "    run_syst = str(datasets_predict[i_dataset][1]['sys'])\n",
    "    # Predict\n",
    "    print('\\nRUNNING Predict '+run_syst)\n",
    "    t0 = time.time()\n",
    "    \n",
    "    gmm = GMMPredict.read_clusters('pamm.pamm', \n",
    "                                   grid_file='pamm.grid', \n",
    "                                   bootstrap_file='pamm.bs')\n",
    "        \n",
    "    print(\"Reading: pamm.pamm\")\n",
    "    print(\"There are {} clusters\".format(np.unique(gmm.pk).shape[0]))\n",
    "    x = datasets_predict[i_dataset][0]\n",
    "    x_ = gmm.predict_proba(x)\n",
    "    labels = np.argmax(x_, axis=1) #.reshape((-1, 1))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('TIME= '+str(np.round(t1-t0, 2))+' s \\n')\n",
    "\n",
    "    # Storing data\n",
    "    cluster_output[run_syst] = labels\n",
    "    grid_cluster[run_syst] = gmm.cluster\n",
    "    prob_output[run_syst] = gmm.p\n",
    "    bootstr_output[run_syst] = gmm.bs\n",
    "    systnames.append(run_syst)\n",
    "    \n",
    "    # output for initial clustering\n",
    "    np.savetxt(run_syst + \"_clusters.txt\", labels.reshape((-1, 1)))\n",
    "    \n",
    "    rates = ClusterRates(datasets_predict[i_dataset][1]['nm_frame'], 'label').calculate_matrix(labels.reshape((-1, 1)))\n",
    "    np.savetxt(run_syst + \"_rates.txt\", rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Structural motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster associations\n",
    "\n",
    "row = 1\n",
    "col = 1\n",
    "fig, ax = plt.subplots(row, col, figsize=(col * 5, row * 4), dpi=100)\n",
    "\n",
    "adjacency, mapping = calculate_adjacency(\n",
    "prob=prob_output['273K'],\n",
    "clusters=grid_cluster['273K'],\n",
    "bootstrap=bootstr_output['273K']\n",
    ")\n",
    "    \n",
    "z = adjancency_dendrogram(adjacency)\n",
    "_ = dendrogram(z, ax=ax, count_sort=True)['leaves']\n",
    "    \n",
    "for k in range(col):\n",
    "    ax.set_yticks([])\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    \n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax.spines[side].set_visible(False)\n",
    "\n",
    "\n",
    "ax.set_ylabel('PAMM DENDROGRAM', size='16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping for the macro structural motifs following the cluster dendrogram.\n",
    "# each entry is a tuple (\"name_of_system\", dict()) where the dict() contains the cluster associations\n",
    "\n",
    "mapping = [\n",
    "    ('273K', {0: [X1,X2,X3],\n",
    "              1: [Y1,Y2,Y3]}),\n",
    "    ('293K', {0: [X1,X2,X3],\n",
    "              1: [Y1,Y2,Y3]}),\n",
    "    ('323K', {0: [X1,X2,X3],\n",
    "              1: [Y1,Y2,Y3]}),\n",
    "    ('333K', {0: [X1,X2,X3],\n",
    "              1: [Y1,Y2,Y3]}),\n",
    "    ('353K', {0: [X1,X2,X3],\n",
    "              1: [Y1,Y2,Y3]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation of the macro clustering and output of the motifs\n",
    "\n",
    "macro_cluster_output = {}\n",
    "rates_macro_clusters = {}\n",
    "\n",
    "for s,macro_cl in enumerate(systnames):\n",
    "    # Macro Cluster\n",
    "    run_syst = macro_cl\n",
    "    print(\"MACRO CLUSTER - \"+run_syst)\n",
    "    \n",
    "    gmm = GMMPredict.read_clusters('pamm.pamm', \n",
    "                                   grid_file='pamm.grid', \n",
    "                                   bootstrap_file='pamm.bs')\n",
    "    \n",
    "    y = datasets_predict[s][0]\n",
    "    y_ = gmm.predict_proba(y)\n",
    "    y__ = np.zeros((y.shape[0], len(mapping[s][1])))\n",
    "    for k, v in mapping[s][1].items():\n",
    "        y__[:, k] = y_[:,v].sum(1)\n",
    "\n",
    "    macro_cluster_output[macro_cl] = np.argmax(y__, axis=1)\n",
    "    np.savetxt(run_syst+'_macro_cluster.dat', np.argmax(y__, axis=1).reshape((-1,1)) )\n",
    "    \n",
    "    rates = ClusterRates(datasets_predict[s][1]['nm_frame'], 'label').calculate_matrix(np.argmax(y__, axis=1))\n",
    "    rates_macro_clusters[macro_cl] = rates\n",
    "    np.savetxt(run_syst+'_macro_rates.dat', rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
